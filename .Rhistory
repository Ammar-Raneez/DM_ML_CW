train_hidden_2 <- na.omit(scaled_exchange_data_hidden_2_shifted[0:400, ])
test_hidden_2 <- na.omit(scaled_exchange_data_hidden_2_shifted[401:500, ])
# Optimal Input vector - AR(1)
scaled_exchange_data_hidden_1_shifted <- shift.column(scaled_exchange_data, columns = "rate", newNames = "rate_1", len = 1, up = F)
scaled_exchange_data_hidden_1_shifted <- scaled_exchange_data_hidden_1_shifted[c(-1)]
train_hidden_1 <- na.omit(scaled_exchange_data_hidden_1_shifted[0:400, ])
test_hidden_1 <- na.omit(scaled_exchange_data_hidden_1_shifted[401:500, ])
times <- as.ts(train_hidden_2[,2])
mlp_model <- mlp(times, hd = c(4), lags = 1)
# import dataset
exchange_data <- read_excel("../../ExchangeUSD.xlsx")
library(readxl)
library(neuralnet)
library(fpp2)
library(ggplot2)
library(Metrics)
library(useful)
library(nnfor)
library(forecast)
# remove unneeded days column
exchange_data <- exchange_data[c(-2)]
# import dataset
exchange_data <- read_excel("../../ExchangeUSD.xlsx")
# convert to time-series, so the date is also in a numeric format
exchange_data <- ts(exchange_data)
exchange_data <- as.data.frame(exchange_data)
# rename col names for easiness
colnames(exchange_data) <- c("date", "rate")
# scale the data
maxs <- apply(exchange_data, 2, max)
mins <- apply(exchange_data, 2, min)
scaled_exchange_data <- as.data.frame(scale(exchange_data, center = mins, scale = maxs - mins))
# plot partial autocorrelation plot to check for optimum order of AR ~ however many orders
# will be tested upon
pacf(scaled_exchange_data[, 2])
# plot the rates to check whether stationary
plot(exchange_data[c(2)])
# Optimal Input vector - AR(1)
scaled_exchange_data_hidden_2_shifted <- shift.column(scaled_exchange_data, columns = "rate", newNames = "rate_1", len = 1, up = F)
scaled_exchange_data_hidden_2_shifted <- scaled_exchange_data_hidden_2_shifted[c(-1)]
train_hidden_2 <- na.omit(scaled_exchange_data_hidden_2_shifted[0:400, ])
test_hidden_2 <- na.omit(scaled_exchange_data_hidden_2_shifted[401:500, ])
# Optimal Input vector - AR(1)
scaled_exchange_data_hidden_1_shifted <- shift.column(scaled_exchange_data, columns = "rate", newNames = "rate_1", len = 1, up = F)
scaled_exchange_data_hidden_1_shifted <- scaled_exchange_data_hidden_1_shifted[c(-1)]
train_hidden_1 <- na.omit(scaled_exchange_data_hidden_1_shifted[0:400, ])
test_hidden_1 <- na.omit(scaled_exchange_data_hidden_1_shifted[401:500, ])
times <- as.ts(train_hidden_2[,2])
mlp_model <- mlp(times, hd = c(4), lags = 1)
View(mlp_model)
times
# import dataset
exchange_data <- read_excel("../../ExchangeUSD.xlsx")
# remove unneeded days column
exchange_data <- exchange_data[c(-2)]
# convert to time-series, so the date is also in a numeric format
exchange_data <- ts(exchange_data)
exchange_data <- as.data.frame(exchange_data)
# rename col names for easiness
colnames(exchange_data) <- c("date", "rate")
# scale the data
maxs <- apply(exchange_data, 2, max)
mins <- apply(exchange_data, 2, min)
scaled_exchange_data <- as.data.frame(scale(exchange_data, center = mins, scale = maxs - mins))
# plot partial autocorrelation plot to check for optimum order of AR ~ however many orders
# will be tested upon
pacf(scaled_exchange_data[, 2])
# plot the rates to check whether stationary
plot(exchange_data[c(2)])
x <- predict(mlp_model, test_hidden_2[,2])
forecasted <- forecast(mlp_model, h=100)
plot(forecasted)
lines(test_hidden_2[, 1])
# import dataset
exchange_data <- read_excel("../../ExchangeUSD.xlsx")
# remove unneeded days column
exchange_data <- exchange_data[c(-2)]
# convert to time-series, so the date is also in a numeric format
exchange_data <- ts(exchange_data)
exchange_data <- as.data.frame(exchange_data)
# rename col names for easiness
colnames(exchange_data) <- c("date", "rate")
# scale the data
maxs <- apply(exchange_data, 2, max)
mins <- apply(exchange_data, 2, min)
scaled_exchange_data <- as.data.frame(scale(exchange_data, center = mins, scale = maxs - mins))
# plot partial autocorrelation plot to check for optimum order of AR ~ however many orders
# will be tested upon
pacf(scaled_exchange_data[, 2])
# plot the rates to check whether stationary
plot(exchange_data[c(2)])
# Optimal Input vector - AR(1)
scaled_exchange_data_hidden_2_shifted <- shift.column(scaled_exchange_data, columns = "rate", newNames = "rate_1", len = 1, up = F)
scaled_exchange_data_hidden_2_shifted <- scaled_exchange_data_hidden_2_shifted[c(-1)]
train_hidden_2 <- na.omit(scaled_exchange_data_hidden_2_shifted[0:400, ])
test_hidden_2 <- na.omit(scaled_exchange_data_hidden_2_shifted[401:500, ])
# Optimal Input vector - AR(1)
scaled_exchange_data_hidden_1_shifted <- shift.column(scaled_exchange_data, columns = "rate", newNames = "rate_1", len = 1, up = F)
scaled_exchange_data_hidden_1_shifted <- scaled_exchange_data_hidden_1_shifted[c(-1)]
train_hidden_1 <- na.omit(scaled_exchange_data_hidden_1_shifted[0:400, ])
test_hidden_1 <- na.omit(scaled_exchange_data_hidden_1_shifted[401:500, ])
times <- as.ts(train_hidden_2[,2])
mlp_model <- mlp(times, hd = c(4), lags = 1)
x <- predict(mlp_model, test_hidden_2[,2])
forecasted <- forecast(mlp_model, h=100)
plot(forecasted)
lines(test_hidden_2[, 1])
lines(times)
lines(scaled_exchange_data)
lines(scaled_exchange_data[,1])
lines(scaled_exchange_data[,2])
plot(forecasted)
lines(scaled_exchange_data[,2])
View(mlp_model)
View(train_hidden_1)
mlp_model[["y"]]
times <- as.ts(exchange_data[,2])
mlp_model <- mlp(times, hd = c(4), lags = 1)
library(nnfor)
mlp_model <- mlp(times, hd = c(4), lags = 1)
View(mlp_model)
mlp_model[["y"]]
mlp_model[["fitted"]]
View(mlp_model)
train_hidden_2
times <- as.ts(train_hidden_2[,2])
mlp_model <- mlp(times, hd = c(4), lags = 1)
View(mlp_model)
mlp_model[["y"]]
median(mlp_model[["y"]])
mlp_model <- mlp(times, hd = c(4), lags = 1, comb = "mean")
View(mlp_model)
mlp_model <- mlp(times, hd = c(4), lags = 1, comb = "mean")
View(mlp_model)
### BEST SINGLE HIDDEN LAYER NETWORK
set.seed(104)
nn_best_single <- neuralnet(rate ~ rate_1, data = mlp_df, hidden = c(4), act.fct = "logistic", err.fct = "sse", lifesign = "full", learningrate = 0.08, rep = 10, linear.output = T)
library(neuralnet)
nn_best_single <- neuralnet(rate ~ rate_1, data = mlp_df, hidden = c(4), act.fct = "logistic", err.fct = "sse", lifesign = "full", learningrate = 0.08, rep = 10, linear.output = T)
nn_best_single <- neuralnet(rate ~ rate_1, data = train_hidden_1, hidden = c(4), act.fct = "logistic", err.fct = "sse", lifesign = "full", learningrate = 0.08, rep = 10, linear.output = T)
nn_best_single.test_prediction <- predict(nn_best_single, test_hidden_1)
plot(nn_best_single)
View(nn_best_single)
View(train_hidden_1)
mlp_model <- mlp(times, hd = c(4), lags = 1, comb = "mean", difforder = 0)
View(mlp_model)
mlp_model <- mlp(times, hd = c(4), lags = 1, comb = "mean", difforder = 0)
View(mlp_model)
mlp_model[["difforder"]]
View(mlp_model)
library(readxl)
library(neuralnet)
library(fpp2)
library(ggplot2)
library(Metrics)
library(useful)
library(nnfor)
library(forecast)
# import dataset
exchange_data <- read_excel("../../ExchangeUSD.xlsx")
# remove unneeded days column
exchange_data <- exchange_data[c(-2)]
# convert to time-series, so the date is also in a numeric format
exchange_data <- ts(exchange_data)
exchange_data <- as.data.frame(exchange_data)
# rename col names for easiness
colnames(exchange_data) <- c("date", "rate")
# scale the data
maxs <- apply(exchange_data, 2, max)
mins <- apply(exchange_data, 2, min)
scaled_exchange_data <- as.data.frame(scale(exchange_data, center = mins, scale = maxs - mins))
# plot partial autocorrelation plot to check for optimum order of AR ~ however many orders
# will be tested upon
pacf(scaled_exchange_data[, 2])
# plot the rates to check whether stationary
plot(exchange_data[c(2)])
# Optimal Input vector - AR(1)
scaled_exchange_data_hidden_2_shifted <- shift.column(scaled_exchange_data, columns = "rate", newNames = "rate_1", len = 1, up = F)
scaled_exchange_data_hidden_2_shifted <- scaled_exchange_data_hidden_2_shifted[c(-1)]
train_hidden_2 <- na.omit(scaled_exchange_data_hidden_2_shifted[0:400, ])
test_hidden_2 <- na.omit(scaled_exchange_data_hidden_2_shifted[401:500, ])
# Optimal Input vector - AR(1)
scaled_exchange_data_hidden_1_shifted <- shift.column(scaled_exchange_data, columns = "rate", newNames = "rate_1", len = 1, up = F)
scaled_exchange_data_hidden_1_shifted <- scaled_exchange_data_hidden_1_shifted[c(-1)]
train_hidden_1 <- na.omit(scaled_exchange_data_hidden_1_shifted[0:400, ])
test_hidden_1 <- na.omit(scaled_exchange_data_hidden_1_shifted[401:500, ])
View(exchange_data)
x <- linscale(exchange_data$rate, minmax=list("mn"=-.8,"mx"=0.8))
View(x)
times <- as.ts(train_hidden_2[,2])
mlp_model <- mlp(times, hd = c(4), lags = 1, comb = "mean", difforder = 0)
View(mlp_model)
scaled_exchange_data <- as.data.frame(scale(exchange_data, center = mins, scale = maxs - mins))
scaled_exchange_data$rate <- linscale(exchange_data$rate, minmax=list("mn"=-.8,"mx"=0.8))
View(scaled_exchange_data)
View(scaled_exchange_data)
scaled_exchange_data <- as.data.frame(scale(exchange_data, center = mins, scale = maxs - mins))
scaled_exchange_data <- linscale(exchange_data$rate, minmax=list("mn"=-.8,"mx"=0.8))
View(scaled_exchange_data)
scaled_exchange_data[["x"]]
mlp_model[["net"]][["covariate"]]
scaled_exchange_data <- as.data.frame(scale(exchange_data, center = mins, scale = maxs - mins))
View(scaled_exchange_data)
scaled_rates <- linscale(exchange_data$rate, minmax=list("mn"=-.8,"mx"=0.8))
scaled_exchange_data$rate <- scaled_rates$x
View(scaled_exchange_data)
# plot partial autocorrelation plot to check for optimum order of AR ~ however many orders
# will be tested upon
pacf(scaled_exchange_data[, 2])
# plot the rates to check whether stationary
plot(exchange_data[c(2)])
# Optimal Input vector - AR(1)
scaled_exchange_data_hidden_2_shifted <- shift.column(scaled_exchange_data, columns = "rate", newNames = "rate_1", len = 1, up = F)
scaled_exchange_data_hidden_2_shifted <- scaled_exchange_data_hidden_2_shifted[c(-1)]
train_hidden_2 <- na.omit(scaled_exchange_data_hidden_2_shifted[0:400, ])
test_hidden_2 <- na.omit(scaled_exchange_data_hidden_2_shifted[401:500, ])
# Optimal Input vector - AR(1)
scaled_exchange_data_hidden_1_shifted <- shift.column(scaled_exchange_data, columns = "rate", newNames = "rate_1", len = 1, up = F)
scaled_exchange_data_hidden_1_shifted <- scaled_exchange_data_hidden_1_shifted[c(-1)]
train_hidden_1 <- na.omit(scaled_exchange_data_hidden_1_shifted[0:400, ])
test_hidden_1 <- na.omit(scaled_exchange_data_hidden_1_shifted[401:500, ])
### BEST SINGLE HIDDEN LAYER NETWORK
set.seed(104)
nn_best_single <- neuralnet(rate ~ rate_1, data = train_hidden_1, hidden = c(4), act.fct = "logistic", err.fct = "sse", lifesign = "full", learningrate = 0.08, rep = 10, linear.output = T)
nn_best_single.test_prediction <- predict(nn_best_single, test_hidden_1)
# plot graph of best network
plot(test_hidden_1[, 1], type = "l", col = "blue", lwd = 2, xlab = "index", ylab = "Rates")
lines(nn_best_single.test_prediction, type = "l", col = "red", lwd = 2)
title("AR(1) Time Series - Single Hidden Layer")
legend("bottomright", legend = c("Actual", "Predicted"), col = c("blue", "red"), bty = "n", cex=1, lwd = 5, text.font = 7)
x <- predict(mlp_model, test_hidden_2[,2])
View(scaled_exchange_data_hidden_1_shifted)
View(train_hidden_1)
len(train_hidden_1)
length(train_hidden_1)
length(train_hidden_1[,1])
forecast(nn_best_single)
forecast(mlp_model)
forecast(mlp_model$net)
forecast(mlp_model$hd)
forecast(mlp_model$lags)
forecast(mlp_model$y)
forecast(mlp_model$fitted)
forecast(mlp_model$sdummy)
forecast(nn_best_single$model.list)
forecast(nn_best_single$call)
forecast(nn_best_single$response)
forecast(nn_best_single$covariate)
forecast(nn_best_single$net.result)
forecast(nn_best_single$result.matrix)
forecast(nn_best_single$startweights)
forecast(nn_best_single$generalized.weights)
forecast(nn_best_single$weights)
forecast(nn_best_single$weights)
forecast(nn_best_single$net.result)
forecast(nn_best_single$exclude)
forecast(nn_best_single$linear.output)
forecast(nn_best_single$act.fct())
forecast(nn_best_single$err.fct())
install.packages("maltese")
library(tidyverse)
library(maltese)
library(neuralnet)
library(dummy)
data("r_enwiki", package = "maltese")
library(maltese)
install.packages("maltese")
install.packages("maltese")
library(dummy)
install.packages("dummy")
library(dummy)
data("r_enwiki", package = "maltese")
remotes::install_github("bearloga/maltese")
install.packages("remotess")
install.packages("remotes")
remotes::install_github("bearloga/maltese")
library(tidyverse)
library(maltese)
library(neuralnet)
library(dummy)
data("r_enwiki", package = "maltese")
head(r_enwiki)
View(r_enwiki)
ggplot(r_enwiki, aes(x = date, y = pageviews)) +
geom_line() +
theme_minimal() +
labs(x = "Date", y = "Pageviews",
title = "Pageviews"
)
split_point <- "2017-02-01"
table(ifelse(r_enwiki$date < split_point, "training set", "testing set"))
normalization_constants <- lapply(
list(median = median, mad = mad, mean = mean, std.dev = sd),
do.call, args = list(x = r_enwiki$pageviews[r_enwiki$date < split_point])
)
View(normalization_constants)
View(normalization_constants)
r_enwiki$normalized <- (r_enwiki$pageviews - normalization_constants$mean)/normalization_constants$std.dev
mlts <- mlts_transform(r_enwiki, date, normalized, p = 21, extras = TRUE, extrasAsFactors = TRUE, granularity = "day")
str(mlts)
es(mlts[, c("mlts_extras_weekday", "mlts_extras_month", "mlts_extras_monthday", "mlts_extras_week"), drop = FALSE])
mlts_dummied <- cbind(mlts, dummy(
mlts[, c("mlts_extras_weekday", "mlts_extras_month", "mlts_extras_monthday", "mlts_extras_week"),
drop = FALSE],
object = mlts_categories, int = TRUE
))
str(mlts_dummied, list.len = 30)
mlts_categories <- categories(mlts[, c("mlts_extras_weekday", "mlts_extras_month", "mlts_extras_monthday", "mlts_extras_week"), drop = FALSE])
mlts_dummied <- cbind(mlts, dummy(
mlts[, c("mlts_extras_weekday", "mlts_extras_month", "mlts_extras_monthday", "mlts_extras_week"),
drop = FALSE],
object = mlts_categories, int = TRUE
))
str(mlts_dummied, list.len = 30)
training_idx <- which(mlts_dummied$dt < split_point)
testing_idx <- which(mlts_dummied$dt >= split_point)
# Train:
set.seed(0)
nn_model <- neuralnet(
nn_formula, mlts_dummied[training_idx, c("y", nn_features)],
linear.output = TRUE, hidden = c(9, 7, 5), algorithm = "sag"
)
tail(r_enwiki, 22)
View(r_enwiki)
library(tidyverse)
library(maltese)
library(neuralnet)
library(dummy)
data("r_enwiki", package = "maltese")
head(r_enwiki)
ggplot(r_enwiki, aes(x = date, y = pageviews)) +
geom_line() +
theme_minimal() +
labs(x = "Date", y = "Pageviews",
title = "Pageviews"
)
split_point <- "2017-02-01"
table(ifelse(r_enwiki$date < split_point, "training set", "testing set"))
normalization_constants <- lapply(
list(median = median, mad = mad, mean = mean, std.dev = sd),
do.call, args = list(x = r_enwiki$pageviews[r_enwiki$date < split_point])
)
r_enwiki$normalized <- (r_enwiki$pageviews - normalization_constants$mean)/normalization_constants$std.dev
mlts <- mlts_transform(r_enwiki, date, normalized, p = 21, extras = TRUE, extrasAsFactors = TRUE, granularity = "day")
str(mlts)
mlts_categories <- categories(mlts[, c("mlts_extras_weekday", "mlts_extras_month", "mlts_extras_monthday", "mlts_extras_week"), drop = FALSE])
mlts_dummied <- cbind(mlts, dummy(
mlts[, c("mlts_extras_weekday", "mlts_extras_month", "mlts_extras_monthday", "mlts_extras_week"),
drop = FALSE],
object = mlts_categories, int = TRUE
))
str(mlts_dummied, list.len = 30)
training_idx <- which(mlts_dummied$dt < split_point)
testing_idx <- which(mlts_dummied$dt >= split_point)
# neuralnet does not support the "y ~ ." formula syntax, so we cheat:
nn_features <- grep("(mlts_lag_[0-9]+)|(mlts_extras_((weekday)|(month)|(monthday)|(week))_.*)", names(mlts_dummied), value = TRUE)
nn_formula <- as.formula(paste("y ~", paste(nn_features, collapse = " + ")))
# Train:
set.seed(0)
nn_model <- neuralnet(
nn_formula, mlts_dummied[training_idx, c("y", nn_features)],
linear.output = TRUE, hidden = c(9, 7, 5), algorithm = "sag"
)
new_data <- rbind(
tail(r_enwiki, 22),
data.frame(
date = seq(
as.Date("2017-05-17"),
as.Date("2017-05-17") + 90,
"day"
),
pageviews = NA,
normalized = NA
)
); rownames(new_data) <- NULL
for (d in 23:nrow(new_data)) {
new_mlts <- mlts_transform(
new_data[(d - 22):d, ],
date, normalized, p = 21,
extras = TRUE, extrasAsFactors = TRUE,
granularity = "day")
new_mlts <- cbind(
new_mlts[-1, ], # don't need to forecast known outcome
dummy(
new_mlts[, c("mlts_extras_weekday", "mlts_extras_month", "mlts_extras_monthday", "mlts_extras_week"),
drop = FALSE],
object = mlts_categories, int = TRUE
)[-1, ]
)
new_data$normalized[d] <- as.numeric(neuralnet::compute(
nn_model, new_mlts[, nn_features])$net.result
)
}
# Forecast on normalized scale:
nn_forecast <- as.numeric(neuralnet::compute(
nn_model, new_mlts[, nn_features])$net.result
)
new_data$pageviews <- (new_data$normalized * normalization_constants$std.dev) + normalization_constants$mean
ggplot(dplyr::filter(r_enwiki, date >= "2016-10-01"),
aes(x = date, y = pageviews)) +
geom_line() +
geom_line(aes(y = pageviews), color = "red",
data = dplyr::filter(new_data, date >= "2017-05-16")) +
theme_minimal() +
labs(x = "Date", y = "Pageviews",
title = "90 days forecast Pageviews"
)
library(readxl)
library(factoextra)
library(NbClust)
#for evaluations
library(caret)
library(ggplot2)
library(cluster)
#remove samples column due to it only being a counter
vehicle_data <- vehicle_data[c(-1)]
#read in the data
vehicle_data <- read_excel("../../vehicles.xlsx")
#data type
class(vehicle_data)
#remove samples column due to it only being a counter
vehicle_data <- vehicle_data[c(-1)]
#data type
class(vehicle_data)
summary(vehicle_data)
# any missing values?
sum(is.na(vehicle_data))
## DATA PREPROCESSING SECTION ##
#normalize the input features
scaled_vehicle_data <- apply(vehicle_data[-c(ncol(vehicle_data))], 2, scale)
#bind the class column back into the normalized data set
scaled_vehicle_data <- cbind(scaled_vehicle_data, vehicle_data[c(ncol(vehicle_data))])
View(scaled_vehicle_data)
#visualize outliers of data set
boxplot(scaled_vehicle_data[-c(ncol(vehicle_data))], las = 2, col = c("lightgreen", "lightblue"), ylab = "Normalized Values")
#Based on box plot, following columns have outliers
have_outliers <- c("Rad.Ra", "Pr.Axis.Ra", "Max.L.Ra", "Pr.Axis.Rect", "Sc.Var.Maxis", "Sc.Var.maxis", "Skew.Maxis", "Skew.maxis", "Kurt.maxis")
#loop through all the columns that have outliers
for (outlier in have_outliers) {
number_of_rows = nrow(scaled_vehicle_data)
#get quantile values - 25th and 75th percentiles
quant <- quantile(scaled_vehicle_data[, outlier], probs = c(0.25, 0.75))
#calculate IQR for the respective columns
iqr <- IQR(scaled_vehicle_data[1:number_of_rows, outlier])
#An outlier is a point below lower quartile and above upper quartile
upper_quartile <- quant[2] + 1.5*iqr
lower_quartile <- quant[1] - 1.5*iqr
#subset the data set, such that the outliers are filtered out
scaled_vehicle_data <- subset(
scaled_vehicle_data,
scaled_vehicle_data[c(-ncol(vehicle_data))][, outlier] > lower_quartile &
scaled_vehicle_data[c(-ncol(vehicle_data))][, outlier] < upper_quartile
)
}
#data set can now be split into input and output
scaled_vehicle_data_inputs <- scaled_vehicle_data[c(-ncol(vehicle_data))]
scaled_vehicle_data_output <- scaled_vehicle_data[c(ncol(vehicle_data))]
#plot box plot again to check whether outliers have been removed
boxplot(scaled_vehicle_data_inputs, las = 2, col = c("lightgreen", "lightblue"), ylab = "Normalized Values")
## KMEANS CLUSTER DEFINITION ##
#Manual
manual_cluster_size <- 4
#Elbow Method
fviz_nbclust(scaled_vehicle_data_inputs, kmeans, method = "wss") + labs(subtitle = "Elbow")
#Silhouette Method
fviz_nbclust(scaled_vehicle_data_inputs, kmeans, method = "silhouette") + labs(subtitle = "Silhouette")
#Gap statistic Method
fviz_nbclust(scaled_vehicle_data_inputs, kmeans, method = "gap_stat", verbose = F) + labs(subtitle = "Gap statistic")
## KMEANS ANALYSIS ##
#set seed for reproducible runs
set.seed(101)
#Manual
manual_kmean <- kmeans(scaled_vehicle_data_inputs, centers = manual_cluster_size, nstart = 50)
clusplot(scaled_vehicle_data, manual_kmean$cluster, color = T, shade = T, labels = 4)
comparison_table_manual <- table(scaled_vehicle_data_output$Class, manual_kmean$cluster)
set.seed(102)
#Elbow
elbow_kmean <- kmeans(scaled_vehicle_data_inputs, centers = 3, nstart = 50)
clusplot(scaled_vehicle_data, elbow_kmean$cluster, color = T, shade = T, labels = 4)
comparison_table_elbow <- table(scaled_vehicle_data_output$Class, elbow_kmean$cluster)
set.seed(103)
#Silhouette & Gap Stat
silhouette_gap_kmean <- kmeans(scaled_vehicle_data_inputs, centers = 2, nstart = 50)
clusplot(scaled_vehicle_data, silhouette_gap_kmean$cluster, color = T, shade = T, labels = 4)
comparison_table_silhouette_gap <- table(scaled_vehicle_data_output$Class, silhouette_gap_kmean$cluster)
#table evaluations for each method
comparison_table_manual
comparison_table_elbow
comparison_table_silhouette_gap
#convert to factors, since confusion matrix expects factors
ground_truth_as_factor <- as.factor(scaled_vehicle_data_output$Class)
predictions_as_factor_manual <- as.factor(manual_kmean$cluster)
predictions_as_factor_elbow <- as.factor(elbow_kmean$cluster)
predictions_as_factor_sg <- as.factor(silhouette_gap_kmean$cluster)
#since the predictions comes out as 1,2,3,4, The categorical data of the ground truth is converted to numerical
ground_truth_as_numeric <- as.numeric(ground_truth_as_factor)
#the data type has to be changed back to factor, since CM can accept only factors
ground_truth_as_factor <- as.factor(ground_truth_as_numeric)
confusionMatrix(data = predictions_as_factor_manual, reference = ground_truth_as_factor, mode = 'prec_recall')
confusionMatrix(data = predictions_as_factor_elbow, reference = ground_truth_as_factor, mode = 'prec_recall')
confusionMatrix(data = predictions_as_factor_sg, reference = ground_truth_as_factor, mode = 'prec_recall')
confusionMatrix(data = predictions_as_factor_elbow, reference = ground_truth_as_factor, mode = 'prec_recall')
# plot confusion matrix for all
confusionMatrix(data = predictions_as_factor_manual, reference = ground_truth_as_factor, mode = 'prec_recall')
elbow_clusters <- elbow_kmean$centers
View(elbow_clusters)
View(manual_kmean)
manual_kmean[["cluster"]]
type.of((new_col_name))
typeof((new_col_name))
typeof(manual_kmean$cluster)
manual_kmean$cluster
